{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bOuADtCmUrK",
        "outputId": "4a361c6f-3287-432b-b455-1b8cd32e08f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gD3febuVmxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "gen_train = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2, \n",
        "                               brightness_range = (0.1, 0.5), horizontal_flip=True)\n",
        "\n",
        "train_data = gen_train.flow_from_directory(\"/content/drive/MyDrive/Dataset\",\n",
        "                                           target_size = (224, 224), batch_size = 32, class_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ_TBULbm3UR",
        "outputId": "65c9c313-b6b4-4100-f0db-52d9e49732d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3350 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "# here i'm going to take input shape, weights and bias from imagenet and include top False means\n",
        "# i want to add input, flatten and output layer by my self\n",
        "\n",
        "vgg16 = VGG16(input_shape = (224, 224, 3), weights = \"imagenet\", include_top = False)"
      ],
      "metadata": {
        "id": "StUThwGam8dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg16.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "VVqvwRQDnBS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "x = layers.Flatten()(vgg16.output)"
      ],
      "metadata": {
        "id": "T6ua4S8CnElI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's add output layers or prediction layer\n",
        "\n",
        "prediction = layers.Dense(units = 8, activation=\"softmax\")(x)\n",
        "\n",
        "# creating a model object\n",
        "\n",
        "model = tf.keras.models.Model(inputs = vgg16.input, outputs=prediction)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdUbRjGEnH-p",
        "outputId": "294c11e7-693e-4e69-b080-41fc3ff3dfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 200712    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,915,400\n",
            "Trainable params: 200,712\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics =[\"accuracy\"])\n",
        "\n",
        "result = model.fit(train_data, epochs = 28, steps_per_epoch=len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXQmd80NnLZ8",
        "outputId": "0de0e146-781d-47d6-fb38-e315df779796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/28\n",
            "105/105 [==============================] - 923s 9s/step - loss: 1.2968 - accuracy: 0.5340\n",
            "Epoch 2/28\n",
            "105/105 [==============================] - 88s 833ms/step - loss: 0.8788 - accuracy: 0.6797\n",
            "Epoch 3/28\n",
            "105/105 [==============================] - 88s 833ms/step - loss: 0.7818 - accuracy: 0.7125\n",
            "Epoch 4/28\n",
            "105/105 [==============================] - 88s 841ms/step - loss: 0.7667 - accuracy: 0.7149\n",
            "Epoch 5/28\n",
            "105/105 [==============================] - 89s 842ms/step - loss: 0.6693 - accuracy: 0.7606\n",
            "Epoch 6/28\n",
            "105/105 [==============================] - 86s 821ms/step - loss: 0.6644 - accuracy: 0.7573\n",
            "Epoch 7/28\n",
            "105/105 [==============================] - 86s 813ms/step - loss: 0.5863 - accuracy: 0.7904\n",
            "Epoch 8/28\n",
            "105/105 [==============================] - 87s 827ms/step - loss: 0.6200 - accuracy: 0.7800\n",
            "Epoch 9/28\n",
            "105/105 [==============================] - 88s 839ms/step - loss: 0.5364 - accuracy: 0.8119\n",
            "Epoch 10/28\n",
            "105/105 [==============================] - 90s 860ms/step - loss: 0.5818 - accuracy: 0.7827\n",
            "Epoch 11/28\n",
            "105/105 [==============================] - 86s 814ms/step - loss: 0.5298 - accuracy: 0.8081\n",
            "Epoch 12/28\n",
            "105/105 [==============================] - 86s 816ms/step - loss: 0.4775 - accuracy: 0.8313\n",
            "Epoch 13/28\n",
            "105/105 [==============================] - 85s 811ms/step - loss: 0.4697 - accuracy: 0.8191\n",
            "Epoch 14/28\n",
            "105/105 [==============================] - 85s 812ms/step - loss: 0.4683 - accuracy: 0.8266\n",
            "Epoch 15/28\n",
            "105/105 [==============================] - 86s 815ms/step - loss: 0.5064 - accuracy: 0.8137\n",
            "Epoch 16/28\n",
            "105/105 [==============================] - 87s 829ms/step - loss: 0.4352 - accuracy: 0.8415\n",
            "Epoch 17/28\n",
            "105/105 [==============================] - 86s 821ms/step - loss: 0.4511 - accuracy: 0.8373\n",
            "Epoch 18/28\n",
            "105/105 [==============================] - 86s 815ms/step - loss: 0.4144 - accuracy: 0.8522\n",
            "Epoch 19/28\n",
            "105/105 [==============================] - 86s 816ms/step - loss: 0.4175 - accuracy: 0.8427\n",
            "Epoch 20/28\n",
            "105/105 [==============================] - 85s 810ms/step - loss: 0.4294 - accuracy: 0.8439\n",
            "Epoch 21/28\n",
            "105/105 [==============================] - 84s 796ms/step - loss: 0.4180 - accuracy: 0.8496\n",
            "Epoch 22/28\n",
            "105/105 [==============================] - 83s 793ms/step - loss: 0.4255 - accuracy: 0.8463\n",
            "Epoch 23/28\n",
            "105/105 [==============================] - 85s 806ms/step - loss: 0.3689 - accuracy: 0.8585\n",
            "Epoch 24/28\n",
            "105/105 [==============================] - 85s 804ms/step - loss: 0.3660 - accuracy: 0.8675\n",
            "Epoch 25/28\n",
            "105/105 [==============================] - 84s 801ms/step - loss: 0.3622 - accuracy: 0.8666\n",
            "Epoch 26/28\n",
            "105/105 [==============================] - 84s 795ms/step - loss: 0.3515 - accuracy: 0.8687\n",
            "Epoch 27/28\n",
            "105/105 [==============================] - 84s 801ms/step - loss: 0.3455 - accuracy: 0.8743\n",
            "Epoch 28/28\n",
            "105/105 [==============================] - 85s 808ms/step - loss: 0.3560 - accuracy: 0.8699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "output_class = [\"batteries\", \"carrot\",\"clothes\", \"e-waste\", \"light blubs\",\"potato\",\"rose\",\"tomato\"]\n",
        "def waste_prediction(new_image):\n",
        "  test_image = image.load_img(new_image, target_size = (224,224))\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(test_image)\n",
        "  plt.show()\n",
        " \n",
        "  test_image = image.img_to_array(test_image) / 255\n",
        "  test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "  predicted_array = model.predict(test_image)\n",
        "  predicted_value = output_class[np.argmax(predicted_array)]\n",
        "  predicted_accuracy = round(np.max(predicted_array) * 100, 2)\n",
        "\n",
        "  print(\"Your waste material is \", predicted_value)"
      ],
      "metadata": {
        "id": "jfubjytQ0_vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"newmodel.h5\")"
      ],
      "metadata": {
        "id": "zaWCktr52CMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naTia93K2mG2",
        "outputId": "a370aca9-bdea-46d2-82c0-e50d59ff4045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "/content\n",
            "9v-battery-isolated-white-sign-energy-24846904.jpg  newmodel.h5   sample_data\n",
            "burnt-light-bulb-12925025.jpg\t\t\t    potato2.jpeg  tomato2.jpeg\n",
            "drive\t\t\t\t\t\t    rose0.jpeg\n",
            "My_TFlite.tflite\t\t\t\t    rose9.jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TF_LITE_MODEL_FILE_NAME = \"tf_lite.tflite\""
      ],
      "metadata": {
        "id": "N_q6JFVO3JXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SFtDQvY3PTX",
        "outputId": "665da2da-4ecf-401e-ba02-b9242cade576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp_z69tzjp/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp_z69tzjp/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    }
  ]
}